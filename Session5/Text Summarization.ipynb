{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37498121",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "This notebook helps to understand how encoder-decoder models may be further finetuned for sequence to sequence tasks such as Summarization. \n",
    "In the example below, we will finetune a [facebook/bart-base](https://huggingface.co/facebook/bart-base) model on a news dataset such as [news-qa-summarization](https://huggingface.co/datasets/glnmario/news-qa-summarization). \n",
    "Information about about [Rouge](https://huggingface.co/spaces/evaluate-metric/rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55898f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5562407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"  # change to \"facebook/bart-large-cnn\" for better quality\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "per_device_train_batch_size = 2  \n",
    "per_device_eval_batch_size = 2  \n",
    "num_epochs = 2  \n",
    "learning_rate = 5e-5\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "388fb11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['story', 'questions', 'answers', 'summary'],\n",
      "    num_rows: 10388\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and inspect schema\n",
    "raw_dataset = load_dataset(\"glnmario/news-qa-summarization\", split=\"train\")\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc92f16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18972900",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "#raw_dataset.select(range(100)).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_val = train_test[\"train\"].train_test_split(\n",
    "    test_size=0.1111, seed=42\n",
    ")\n",
    "\n",
    "# 3) Rebuild a DatasetDict with 3 splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\":      train_val[\"train\"],\n",
    "    \"validation\": train_val[\"test\"],\n",
    "    \"test\":       train_test[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79b1330b-4c9c-437e-8ee7-89e51379fbbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8310, 1039, 1039)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"train\"]), len(dataset[\"validation\"]), len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f8014706-be7b-4ec2-8eda-fcaf156f10bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['story', 'questions', 'answers', 'summary'],\n",
       "    num_rows: 8310\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "adfaeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50265\n",
      "Model params (M): 139.42\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Model params (M):\", round(model.num_parameters() / 1e6, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8560e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # inputs: articles\n",
    "    inputs = examples[\"story\"]\n",
    "    # targets: summaries\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # or \"longest\" for on-the-fly padding\n",
    "    )\n",
    "\n",
    "    # tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # Replace padding token id in labels with -100 so they are ignored by loss\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "    labels_ids = [\n",
    "        [(lid if lid != tokenizer.pad_token_id else -100) for lid in label]\n",
    "        for label in labels_ids\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beb08e16-009e-43c0-ac86-ee15665530f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['story', 'questions', 'answers', 'summary']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = list(dataset[\"train\"].features)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "525c2bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325957f8f98e402ea6bc2b95748d2410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/sn23a250/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(preprocess_function, batched=True, remove_columns=list(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67d580dd-86b6-4aa7-9756-cb21e33d312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8310\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1039\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1039\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fa79787",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9b530cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "#np.where(condition, A, B) means:\n",
    "#if condition is True, take from A\n",
    "#else (False), take from B\n",
    "\n",
    "def compute_metrics(eval_pred): # eval_pred is a tuple of predictions, labels\n",
    "    predictions, labels = eval_pred\n",
    "    # replace -100 back to pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id) \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)  \n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        predictions, skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(\n",
    "        decoded_preds, decoded_labels\n",
    "    )\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # average Rouge-L / Rouge-1 / Rouge-2\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "    # also track average generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(p != tokenizer.pad_token_id) for p in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d46b1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bart-newsqa-sum\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  \n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    gradient_accumulation_steps=8,   \n",
    "    fp16=True,                      \n",
    "    report_to=\"none\",               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5572cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/38131528/ipykernel_2557520/2588448291.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1560' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1560/1560 12:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.191200</td>\n",
       "      <td>1.929808</td>\n",
       "      <td>35.400000</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>24.480000</td>\n",
       "      <td>33.140000</td>\n",
       "      <td>61.423484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.968800</td>\n",
       "      <td>1.895033</td>\n",
       "      <td>36.480000</td>\n",
       "      <td>14.840000</td>\n",
       "      <td>25.470000</td>\n",
       "      <td>34.220000</td>\n",
       "      <td>60.738210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.858600</td>\n",
       "      <td>1.880833</td>\n",
       "      <td>36.140000</td>\n",
       "      <td>14.460000</td>\n",
       "      <td>25.140000</td>\n",
       "      <td>33.880000</td>\n",
       "      <td>62.033686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/sn23a250/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1560, training_loss=2.079087428557567, metrics={'train_runtime': 761.0724, 'train_samples_per_second': 32.756, 'train_steps_per_second': 2.05, 'total_flos': 1.52007299039232e+16, 'train_loss': 2.079087428557567, 'epoch': 3.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77d272f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bart-newsqa-sum-final/tokenizer_config.json',\n",
       " 'bart-newsqa-sum-final/special_tokens_map.json',\n",
       " 'bart-newsqa-sum-final/vocab.json',\n",
       " 'bart-newsqa-sum-final/merges.txt',\n",
       " 'bart-newsqa-sum-final/added_tokens.json',\n",
       " 'bart-newsqa-sum-final/tokenizer.json')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.save_model(\"bart-newsqa-sum-final\")\n",
    "tokenizer.save_pretrained(\"bart-newsqa-sum-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d2a0c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"bart-newsqa-sum-final\",\n",
    "    tokenizer=\"bart-newsqa-sum-final\",\n",
    "    device=0,  # or -1 for CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e2458ed-464f-4756-8f53-991b24655141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Article: 'SINDH KALAY', England (CNN) -- The aroma of freshly baking flatbread wafts through the air as a unit of British soldiers position themselves for a quick patrol around the village of Sindh Kalay. A British soldier on patrol in the mock Afghan village of Sindh Kalay. Market vendors hawk grapes and melons, as a group of village elders sit smoking water pipes and suspicious-looking men lurk beside battered motorcycles. What should the soldiers do? Conduct a weapons search? Approach the village elders first? In the complex political and cultural terrain of Afghanistan, what is the best course of action? Except this is not Afghanistan. It's Norfolk, England. Instead of the Hindu Kush mountains, it is the green ladscape and tidy farmhouses of the English countryside that stretch out behind them. Welcome to the British Army's state-of-the art training ground. It cost more than $20 million to build and every British soldier serving in Afghanistan will do his or her training here. \"I think it's the closest thing you are going to get short of being in Afghanistan itself,\" says Col. David Colthup of the 2nd Battalion of the Yorkshire Regiment. His troops have already served one tour of duty in Afghanistan's Helmand province and are training for another. British troops serving in Helmand province are tasked with mentoring and training Afghan security forces. Not an easy job in a Taliban stronghold and Afghanistan's center of opium production. \"Ultimately, a soldier joins the army and trains to fight. That's what a soldier trains to do. But today, it's a much, much more complex environment,\" explains Colthup. \"The business of being able to interact either through an interpreter or through Afghan security forces, whether they are police or army. And to understand how the people operate and how we can interact better with them. Because ultimately, that's what it's about,\" he says. The most distinctive features of Sindh Kalay are the high three-meter walls that make up the village compound, creating narrow alleyways difficult for troops to patrol. The village is staffed with Afghan asylum-seekers, many of whom have fled the Taliban. They play the roles of market vendors, village elders and sometimes Afghan security forces. Several Afghan women are also on hand, useful for training British soldiers on the religious and cultural sensitivities of entering an Afghan home.  Watch British troops training in mock Afghan village » The Taliban insurgents are played by Nepalese Ghurkha soldiers authorized to handle weapons. They play their roles silently, unable to partake in the Pashtun banter among the Afghans. Fazel Beria is also an asylum-seeker from Afghanistan. He is responsible for recruiting and for creating the sights and smells of Sindh Kalay and is easily identifiable as the only Afghan in the market in Western clothes. He beams with pride walking down the bazaar and clearly relishes his role in training the British Army. \"Everything with the culture comes up with the issue of hearts and minds,\" he explains. \"If you want to win that, you need to know about their culture. You need to respect their culture, their religion and their way of life.\" He gives high marks to the soldiers training so far. After each exercise, the Afghan actors talk directly to the soldiers about what went wrong and what went right. Sometimes, it's the little things that count. \"Yes, there have been quite a lot of surprises,\" Beria says. Like Afghan will sit cross legged for hours. \"The British soldier cannot do that,\" he laughs. \"The Afghan will be sitting very comfortable and the British soldier is not. So, they have to get used to it.\"  See photos of British troops on patrol in Sindh Kalay -- and for real in Afghanistan » Previously, the army trained on farmhouses and in urban neighborhoods that resembled Northern Ireland more than Afghanistan. But Sindh Kalay does more than mimic the physical reality of Afghanistan. It also mirrors the changing tactics on the ground. Troops are grilled in\n",
      "\n",
      "\n",
      " Summary:British soldiers train in mock Afghan village of Sindh Kalay .\n",
      "The village is staffed with Afghan asylum-seekers, many of whom have fled the Taliban .\n",
      "Taliban insurgents are played by Nepalese Ghurkha soldiers authorized to handle weapons .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "article_sample = raw_dataset[0][\"story\"]\n",
    "print(f\"\\n\\n Article: {article_sample}\")\n",
    "summary = (summarizer(article_sample, max_length=128, min_length=20, do_sample=False)[0][\"summary_text\"])\n",
    "print(f\"\\n\\n Summary:{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e340fd2e-3be0-4271-8d79-d6fceedcb5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Article: Low blood sugar detected by speaking into a smartphone. Low blood sugar (hypoglycemia) is a critical diabetes-related condition. Researchers at the Inselspital, Bern University Hospital and the University of Bern have now shown for the first time that the human voice can even reveal early signs of hypoglycemia. Recordings made with the microphone of an ordinary smartphone and analyzed using artificial intelligence could make diabetes management safer and easier in the future.  Low blood sugar, medically known as hypoglycemia, is one of the most common and dangerous acute complications of diabetes. Within minutes, it can lead to dizziness, confusion, loss of consciousness, or even life-threatening situations. Despite modern glucose sensors, it is often difficult to recognize impending hypoglycemia in time. Yet the human voice is recognized to be a sensitive mirror of the body: it changes when we are tired, stressed, or ill; and, as it now turns out, also when blood sugar drops. The voice as a warning signal Researchers at the Inselspital, Bern University Hospital and the University of Bern, together with international partners, have shown for the first time that hypoglycemia can be reliably detected based on characteristic changes in the voice. All that was needed were voice recordings made with the microphone of a commercially available smartphone which were then evaluated using a machine-learning algorithm. In all, 22 people with type 1 diabetes took part in two clinical studies. Under strictly controlled conditions, the participants' blood sugar levels were, on the one hand, adjusted to a normal level and, on the other, deliberately lowered to induce hypoglycemia. Within these two phases, the participants spoke into the microphone of an ordinary smartphone in a quiet room. They read texts aloud, described images, held vowels or repeated syllable sequences in rapid succession. This resulted in a total of 540 voice recordings taken at normal or at low blood sugar levels. The researchers then evaluated the audio recordings using a machine-learning algorithm. The AI analyzed subtle differences in the voice, such as pitch, volume, resonance, clarity, and sound dynamics. On this basis, its ability to detect hypoglycemia was very reliable. The AI achieved its best results when the participants read aloud, where it correctly detected hypoglycemia in around 90 percent of cases. When repeating short syllables, the accuracy was around 87 percent. Simple technology with great potential The study proves for the first time that changes in the voice can indicate an acute medical problem. «Our findings clearly show that the voice can provide important clues about a person's state of health,» says Prof. Christoph Stettler, Director and Chief Physician of the Department of Diabetes, Endocrinology, Nutritional Medicine and Metabolism at the Inselspital Bern (UDEM), the study's last author. «Using an ordinary smartphone and artificial intelligence, hypoglycemia can be detected at an early stage without the need for additional devices.» Dr. Vera Lehmann, clinical research physician and the study's first author, also emphasizes the significance of the results: «We were able to show that an ordinary smartphone is sufficient to detect physiological changes that people are themselves sometimes unaware of. This opens up completely new possibilities for ways in which technology can help prevent dangerous situations in the future.» Given the widespread use of smartphones, this approach could improve the detection and prevention of hypoglycemia worldwide, especially in regions where modern glucose sensors are not widely available. However, the researchers emphasize that the method is intended to complement existing technologies, not replace them. Bern as a hub for innovative diabetes research The research team at the Inselspital and the University of Bern is one of the world's leading groups in the field of AI-supported diabetes research. In an earlier study, the research group has already shown that behavior while driving a car can indicate low blood sugar levels. With this current study, the researchers have added a new dimension to the spectrum: the voice as a biomarker for acute metabolic imbalance. Next steps toward everyday use In further studies, the researchers now want to test whether voice analysis is also effective in everyday speech situations, such as when using voice assistants like Siri or Alexa. If the approach proves successful, then in the future, simple voice commands could help early detection of dangerously low blood sugar levels, making life safer for people with diabetes\n",
      "\n",
      "\n",
      " Summary:Low blood sugar detected by speaking into a smartphone .\n",
      "Human voice can even reveal early signs of hypoglycemia .\n",
      "Researchers at the Inselspital, Bern University Hospital and the University of Bern have now shown for the first time that the human voice can be reliably detected .\n",
      "Humans are recognized to be a sensitive mirror of the body .\n"
     ]
    }
   ],
   "source": [
    "# source of the article = https://mediarelations.unibe.ch/media_releases/2025/media_releases_2025/low_blood_sugar_detected_by_speaking_into_a_smartphone/index_eng.html\n",
    "article_sample = \"Low blood sugar detected by speaking into a smartphone. Low blood sugar (hypoglycemia) is a critical diabetes-related condition. Researchers at the Inselspital, Bern University Hospital and the University of Bern have now shown for the first time that the human voice can even reveal early signs of hypoglycemia. Recordings made with the microphone of an ordinary smartphone and analyzed using artificial intelligence could make diabetes management safer and easier in the future.  Low blood sugar, medically known as hypoglycemia, is one of the most common and dangerous acute complications of diabetes. Within minutes, it can lead to dizziness, confusion, loss of consciousness, or even life-threatening situations. Despite modern glucose sensors, it is often difficult to recognize impending hypoglycemia in time. Yet the human voice is recognized to be a sensitive mirror of the body: it changes when we are tired, stressed, or ill; and, as it now turns out, also when blood sugar drops. The voice as a warning signal Researchers at the Inselspital, Bern University Hospital and the University of Bern, together with international partners, have shown for the first time that hypoglycemia can be reliably detected based on characteristic changes in the voice. All that was needed were voice recordings made with the microphone of a commercially available smartphone which were then evaluated using a machine-learning algorithm. In all, 22 people with type 1 diabetes took part in two clinical studies. Under strictly controlled conditions, the participants' blood sugar levels were, on the one hand, adjusted to a normal level and, on the other, deliberately lowered to induce hypoglycemia. Within these two phases, the participants spoke into the microphone of an ordinary smartphone in a quiet room. They read texts aloud, described images, held vowels or repeated syllable sequences in rapid succession. This resulted in a total of 540 voice recordings taken at normal or at low blood sugar levels. The researchers then evaluated the audio recordings using a machine-learning algorithm. The AI analyzed subtle differences in the voice, such as pitch, volume, resonance, clarity, and sound dynamics. On this basis, its ability to detect hypoglycemia was very reliable. The AI achieved its best results when the participants read aloud, where it correctly detected hypoglycemia in around 90 percent of cases. When repeating short syllables, the accuracy was around 87 percent. Simple technology with great potential The study proves for the first time that changes in the voice can indicate an acute medical problem. «Our findings clearly show that the voice can provide important clues about a person's state of health,» says Prof. Christoph Stettler, Director and Chief Physician of the Department of Diabetes, Endocrinology, Nutritional Medicine and Metabolism at the Inselspital Bern (UDEM), the study's last author. «Using an ordinary smartphone and artificial intelligence, hypoglycemia can be detected at an early stage without the need for additional devices.» Dr. Vera Lehmann, clinical research physician and the study's first author, also emphasizes the significance of the results: «We were able to show that an ordinary smartphone is sufficient to detect physiological changes that people are themselves sometimes unaware of. This opens up completely new possibilities for ways in which technology can help prevent dangerous situations in the future.» Given the widespread use of smartphones, this approach could improve the detection and prevention of hypoglycemia worldwide, especially in regions where modern glucose sensors are not widely available. However, the researchers emphasize that the method is intended to complement existing technologies, not replace them. Bern as a hub for innovative diabetes research The research team at the Inselspital and the University of Bern is one of the world's leading groups in the field of AI-supported diabetes research. In an earlier study, the research group has already shown that behavior while driving a car can indicate low blood sugar levels. With this current study, the researchers have added a new dimension to the spectrum: the voice as a biomarker for acute metabolic imbalance. Next steps toward everyday use In further studies, the researchers now want to test whether voice analysis is also effective in everyday speech situations, such as when using voice assistants like Siri or Alexa. If the approach proves successful, then in the future, simple voice commands could help early detection of dangerously low blood sugar levels, making life safer for people with diabetes\"\n",
    "print(f\"\\n\\n Article: {article_sample}\")\n",
    "summary = (summarizer(article_sample, max_length=128, min_length=20, do_sample=False)[0][\"summary_text\"])\n",
    "print(f\"\\n\\n Summary:{summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe61e1-7c77-44ba-9e1d-ddaca88c6dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c037bc64",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "1. Choose a different news story to summarize.\n",
    "2. Compare the result with another encoder-decoder model such as T5. Which performs better? Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f39403",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "conda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
