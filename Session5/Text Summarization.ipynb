{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37498121",
   "metadata": {},
   "source": [
    "# Text Summarization\n",
    "\n",
    "This notebook helps to understand how encoder-decoder models may be further finetuned for sequence to sequence tasks such as Summarization. \n",
    "In the example below, we will finetune a `facebook/bart-base` model on a news dataset such as [news-qa-summarization](https://huggingface.co/datasets/glnmario/news-qa-summarization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55898f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5562407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"  # change to \"facebook/bart-large-cnn\" for better quality\n",
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "per_device_train_batch_size = 2  \n",
    "per_device_eval_batch_size = 2  \n",
    "num_epochs = 2  \n",
    "learning_rate = 5e-5\n",
    "seed = 42\n",
    "\n",
    "set_seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fb11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 715/715 [00:00<00:00, 2.67kB/s]\n",
      "Downloading data: 100%|██████████| 40.7M/40.7M [00:06<00:00, 6.40MB/s]\n",
      "Generating train split: 100%|██████████| 10388/10388 [00:00<00:00, 35749.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['story', 'questions', 'answers', 'summary'],\n",
      "        num_rows: 10388\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and inspect schema\n",
    "raw_dataset = load_dataset(\"glnmario/news-qa-summarization\")\n",
    "print(raw_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc92f16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'story': '\\'SINDH KALAY\\', England (CNN) -- The aroma of freshly baking flatbread wafts through the air as a unit of British soldiers position themselves for a quick patrol around the village of Sindh Kalay. A British soldier on patrol in the mock Afghan village of Sindh Kalay. Market vendors hawk grapes and melons, as a group of village elders sit smoking water pipes and suspicious-looking men lurk beside battered motorcycles. What should the soldiers do? Conduct a weapons search? Approach the village elders first? In the complex political and cultural terrain of Afghanistan, what is the best course of action? Except this is not Afghanistan. It\\'s Norfolk, England. Instead of the Hindu Kush mountains, it is the green ladscape and tidy farmhouses of the English countryside that stretch out behind them. Welcome to the British Army\\'s state-of-the art training ground. It cost more than $20 million to build and every British soldier serving in Afghanistan will do his or her training here. \"I think it\\'s the closest thing you are going to get short of being in Afghanistan itself,\" says Col. David Colthup of the 2nd Battalion of the Yorkshire Regiment. His troops have already served one tour of duty in Afghanistan\\'s Helmand province and are training for another. British troops serving in Helmand province are tasked with mentoring and training Afghan security forces. Not an easy job in a Taliban stronghold and Afghanistan\\'s center of opium production. \"Ultimately, a soldier joins the army and trains to fight. That\\'s what a soldier trains to do. But today, it\\'s a much, much more complex environment,\" explains Colthup. \"The business of being able to interact either through an interpreter or through Afghan security forces, whether they are police or army. And to understand how the people operate and how we can interact better with them. Because ultimately, that\\'s what it\\'s about,\" he says. The most distinctive features of Sindh Kalay are the high three-meter walls that make up the village compound, creating narrow alleyways difficult for troops to patrol. The village is staffed with Afghan asylum-seekers, many of whom have fled the Taliban. They play the roles of market vendors, village elders and sometimes Afghan security forces. Several Afghan women are also on hand, useful for training British soldiers on the religious and cultural sensitivities of entering an Afghan home.  Watch British troops training in mock Afghan village » The Taliban insurgents are played by Nepalese Ghurkha soldiers authorized to handle weapons. They play their roles silently, unable to partake in the Pashtun banter among the Afghans. Fazel Beria is also an asylum-seeker from Afghanistan. He is responsible for recruiting and for creating the sights and smells of Sindh Kalay and is easily identifiable as the only Afghan in the market in Western clothes. He beams with pride walking down the bazaar and clearly relishes his role in training the British Army. \"Everything with the culture comes up with the issue of hearts and minds,\" he explains. \"If you want to win that, you need to know about their culture. You need to respect their culture, their religion and their way of life.\" He gives high marks to the soldiers training so far. After each exercise, the Afghan actors talk directly to the soldiers about what went wrong and what went right. Sometimes, it\\'s the little things that count. \"Yes, there have been quite a lot of surprises,\" Beria says. Like Afghan will sit cross legged for hours. \"The British soldier cannot do that,\" he laughs. \"The Afghan will be sitting very comfortable and the British soldier is not. So, they have to get used to it.\"  See photos of British troops on patrol in Sindh Kalay -- and for real in Afghanistan » Previously, the army trained on farmhouses and in urban neighborhoods that resembled Northern Ireland more than Afghanistan. But Sindh Kalay does more than mimic the physical reality of Afghanistan. It also mirrors the changing tactics on the ground. Troops are grilled in',\n",
       " 'questions': ['Where were they being deployed to?',\n",
       "  'What does the village train soldiers to understand?',\n",
       "  'Where do British soldiers train?',\n",
       "  'Where will British troops be deployed?',\n",
       "  'Where did the soldiers train?',\n",
       "  'What are soldiers being trained to understand?',\n",
       "  'Who play the part of Taliban militants?',\n",
       "  'What does the village feature?',\n",
       "  'Who plays Taliban militants during mock Afghan village?',\n",
       "  'What does the village train the soldiers to do?',\n",
       "  'Who plays the role of \"Taliban militants\"?',\n",
       "  'What do the mock Afghan villages contain?',\n",
       "  'What did British soldiers do before being deployed to Afghanistan?'],\n",
       " 'answers': [[\"Afghanistan's Helmand province\"],\n",
       "  ['how the people operate and how we can interact better with them.'],\n",
       "  ['mock Afghan village of Sindh Kalay.'],\n",
       "  ['around the village of Sindh Kalay.'],\n",
       "  ['Norfolk, England.'],\n",
       "  ['how the people operate and how we can interact better with them.'],\n",
       "  ['Nepalese Ghurkha soldiers'],\n",
       "  ['high three-meter walls'],\n",
       "  ['Nepalese Ghurkha soldiers'],\n",
       "  ['religious and cultural sensitivities'],\n",
       "  ['Nepalese Ghurkha soldiers authorized to handle weapons.'],\n",
       "  [\"British Army's state-of-the art training ground.\"],\n",
       "  ['training']],\n",
       " 'summary': 'British soldiers train in mock Afghan village before deployment to Afghanistan .\\nVillage features Afghan asylum-seekers as vendors, elders, Afghan forces .\\nTaliban militants are played by Nepalese Ghurkha soldiers .\\nVillage trains soldiers to understand Afghan customs, respect Afghan culture .'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18972900",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"glnmario/news-qa-summarization\")\n",
    "\n",
    "train_test = raw_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.1, seed=42\n",
    ")\n",
    "\n",
    "train_val = train_test[\"train\"].train_test_split(\n",
    "    test_size=0.1111, seed=42\n",
    ")\n",
    "\n",
    "# 3) Rebuild a DatasetDict with 3 splits\n",
    "dataset = DatasetDict({\n",
    "    \"train\":      train_val[\"train\"],\n",
    "    \"validation\": train_val[\"test\"],\n",
    "    \"test\":       train_test[\"test\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfaeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50265\n",
      "Model params (M): 139.42\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Model params (M):\", round(model.num_parameters() / 1e6, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8560e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # inputs: articles\n",
    "    inputs = examples[\"story\"]\n",
    "    # targets: summaries\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # or \"longest\" for on-the-fly padding\n",
    "    )\n",
    "\n",
    "    # tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # Replace padding token id in labels with -100 so they are ignored by loss\n",
    "    labels_ids = labels[\"input_ids\"]\n",
    "    labels_ids = [\n",
    "        [(lid if lid != tokenizer.pad_token_id else -100) for lid in label]\n",
    "        for label in labels_ids\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525c2bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8310 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sukan\\PycharmProjects\\CAS_NLP_M4\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8310/8310 [00:32<00:00, 252.40 examples/s]\n",
      "Map: 100%|██████████| 1039/1039 [00:04<00:00, 238.51 examples/s]\n",
      "Map: 100%|██████████| 1039/1039 [00:03<00:00, 269.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa79787",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b530cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.14kB [00:00, 3.02MB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [p.strip() for p in preds]\n",
    "    labels = [l.strip() for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # replace -100 back to pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(\n",
    "        predictions, skip_special_tokens=True\n",
    "    )\n",
    "    decoded_labels = tokenizer.batch_decode(\n",
    "        labels, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(\n",
    "        decoded_preds, decoded_labels\n",
    "    )\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # average Rouge-L / Rouge-1 / Rouge-2\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "    # also track average generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(p != tokenizer.pad_token_id) for p in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d46b1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"bart-newsqa-sum\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  \n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    gradient_accumulation_steps=8,   \n",
    "    fp16=True,                      \n",
    "    report_to=\"none\",               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sukan\\AppData\\Local\\Temp\\ipykernel_55684\\2588448291.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/1560 01:06 < 1:25:29, 0.30 it/s, Epoch 0.04/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d272f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model(\"bart-newsqa-sum-final\")\n",
    "tokenizer.save_pretrained(\"bart-newsqa-sum-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"bart-newsqa-sum-final\",\n",
    "    tokenizer=\"bart-newsqa-sum-final\",\n",
    "    device=0,  # or -1 for CPU\n",
    ")\n",
    "\n",
    "sample = raw_dataset[\"train\"][0][\"story\"]\n",
    "print(summarizer(sample, max_length=128, min_length=20, do_sample=False)[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037bc64",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "1. Choose a different news story to summarize.\n",
    "2. Compare the result with another encoder-decoder model such as T5. Which performs better? Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f39403",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
