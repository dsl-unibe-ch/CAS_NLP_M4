{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7450b57b",
      "metadata": {},
      "source": [
        "# Introduction to Transformers\n",
        "\n",
        "### Objective:\n",
        "Familiarise yourself with the Huggingface transformer components such as tokenizers, models and try out some basic applications with Pipelines.\n",
        "\n",
        "1. [Huggingface Models](https://huggingface.co/models) : Familiarise yourself on how to select models for certain tasks, languages.\n",
        "2. [Huggingface Datasets](https://huggingface.co/datasets) : Explore the different datasets and observe how certain datasets are suitable for certain tasks.\n",
        "3. [Hugginface Documentation](https://huggingface.co/docs) : Familiarise yourself with the documentation of Huggingface. \n",
        "4. [Huggingface LLM Course](https://huggingface.co/learn/llm-course/chapter1/1) **[Recommended Self-Study]**\n",
        "\n",
        "\n",
        "#### Models\n",
        "Models are transformer based and can be encoder, decoder or encoder-decoder categories.\n",
        "\n",
        "#### Tokenizers\n",
        "\n",
        "The tokenizer is responsible for breaking down the input sequence into a set of tokens. They return a list of input_ids, token_type_ids and attention_mask\n",
        "1. **input_ids** are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
        "2. **attention_mask** is a binary tensor which indicates to the model which tokens should be attended to, and which should not (padded values are marked as 0).\n",
        "3. **token_type_ids** are useful for applications where more than one sequences are present such as sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. \n",
        "\n",
        "#### Pipelines\n",
        "Pipelines are an abstraction under which a model is connected including the required preprocessing and postprocessing steps, allowing us to directly input any text and get a suitable answer. More information on the type of pipelines [here.](https://github.com/huggingface/transformers/tree/main/src/transformers/pipelines). Using pipelines without a specific model will cause a default model to be fetched. For reproducibility, it is recommended to specify a model suitable for the task. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "81f439db",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/storage/homefs/sn23a250/CAS_NLP_M4/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a7d20394",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b6b9d631",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"I love the CAS in NLP a lot!!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8691ea80",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2293, 1996, 25222, 1999, 17953, 2361, 1037, 2843, 999, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b01ad9",
      "metadata": {},
      "source": [
        "The tokenizer creates from input sequence a list of input_ids, token_type_ids and attention_mask\n",
        "\n",
        "input_ids are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
        "attention_mask is a binary tensor which indicates to the model which tokens should be attended to, and which should not (padded values are marked as 0).\n",
        "token_type_ids are useful for applications where more than one sequences are present such as sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9c6d40b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2293, 1996, 25222, 1999, 17953, 2361, 1037, 2843, 999, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcbb87d",
      "metadata": {},
      "source": [
        "## To Do\n",
        "What do the different functions do?\n",
        "1. tokenizer()\n",
        "2. tokenizer.tokenize()\n",
        "3. tokenizer.encode()\n",
        "4. tokenizer.encode_plus()\n",
        "5. tokenizer.batch_encode_plus()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db120695",
      "metadata": {},
      "source": [
        "## [Padding and Truncation](https://huggingface.co/docs/transformers/pad_truncation) strategies\n",
        "Padding or truncation is needed as sequences in a batch can vary in length.\n",
        "While there are different strategies as you can see in the link above, a common way is to pad the batch to the length of the longest sequence (in the batch) and truncating to the maximum length a model can accept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5106b0c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1045, 2293, 1996, 25222, 1999, 17953, 2361, 1037, 2843, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.batch_encode_plus([text], padding = \"max_length\", max_length = tokenizer.model_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b381c2c0",
      "metadata": {},
      "source": [
        "## Huggingface [Pipelines](https://huggingface.co/docs/transformers/main/en/quicktour)\n",
        "Pipelines are workflows that transform a sequence of input data with the necessary preprocessing and postprocessing steps to get a suitable output as per the chosen task.\n",
        "\n",
        "### Text Generation Pipeline\n",
        "The text generation pipeline continues generating new text given another text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b2519542",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[{'generated_text': 'Sun rises in the evening, the waters are rushing, and the sun is rising again.\\n\\nIt is the day that your father told you about. There are many things, but one thing is certain: for this day will not last for long.\\n\\nThe day will not last long for long.\\n\\nYou are a child. You will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be the same as before.\\n\\nYou will not be'}],\n",
              " [{'generated_text': 'Climate change is causing more harm to the planet than good.\\n\\nWe know that climate change is causing more harm to the planet than good.\\n\\nBut what does this mean for our country?\\n\\nAs we look to the future of our economy and the future of our kids, what about climate change?\\n\\nA new report from the Pew Research Center finds that the impact of climate change on the U.S. economy is higher for the rich than for the poor.\\n\\nAccording to the report, rising temperatures and rising seas are causing more damage to our planet than good.\\n\\n\"The average American household\\'s wealth is the largest in the developed world,\" said Fred Singer, a senior fellow at the Pew Research Center. \"That\\'s not a bad thing.\"\\n\\nAccording to the report:\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s wealth is the largest in the developed world.\\n\\nThe average family\\'s'}],\n",
              " [{'generated_text': 'US and Europe are now in the grip of a violent insurgency, with the Taliban ruling Afghanistan.\\n\\n\"We are seeing the blood of our sons, our wives and our children being shed,\" said Sohail Jazayeri, a teacher at Haji Khan school, where he works, but which has been attacked three times. The Taliban have also taken up arms on the ground, he said, adding that he has had to flee from the school.\\n\\n\"The Taliban are coming for us, and they won\\'t let us go. They will not let us leave our house, and they will not let us go,\" he told Reuters.\\n\\nThe Taliban have been accused of having a \"humanitarian hand\" in the war against them, but the group has been accused of being a terrorist group in a number of other cases.\\n\\nThe fighting has also brought with it the return of government officials and government officials who were killed or injured in what is believed to be the first such attacks by the Islamic State in Iraq and the Levant, or ISIL.'}]]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_generation_pipe = pipeline(\"text-generation\", model = \"gpt2\")\n",
        "text_list = [\"Sun rises in the\",\n",
        "             \"Climate change is causing\",\n",
        "             \"US and Europe are\"]\n",
        "text_generation_pipe(text_list, pad_token_id=50256) # Decoder models such as GPT2 do not have a built in token. For a list of sequences of varying lengths, the model needs to pad the sequences so that they are the same length. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9384d298",
      "metadata": {},
      "source": [
        "### Question answering Pipeline\n",
        "\n",
        "Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context (similar to text generation), while others need context.\n",
        "The example below is extractive QA where the model requires both context and an input question to answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c0ecfd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.8546791672706604,\n",
              " 'start': 37,\n",
              " 'end': 55,\n",
              " 'answer': 'University of Bern'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question_answer_pipeline = pipeline(\"question-answering\")\n",
        "question_answer_pipeline(\n",
        "    question=\"Where do I work?\", #currently\n",
        "    context=\"My name is Sukanya and I work at the University of Bern\" # \"My name is Sukanya and I have worked at the Universities of Neuchatel and FFHS before arriving at the University of Bern\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5db571bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.4752790033817291,\n",
              " 'start': 102,\n",
              " 'end': 120,\n",
              " 'answer': 'University of Bern'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "question_answer_pipeline = pipeline(\"question-answering\", model = model_name, tokenizer = model_name)\n",
        "question_answer_pipeline(\n",
        "    question=\"Where do I work?\",  #currently\n",
        "    context=\"My name is Sukanya and I have worked at the Universities of Neuchatel and FFHs before arriving at the University of Bern\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733c784d",
      "metadata": {},
      "source": [
        "### Masked language modelling\n",
        "\n",
        "Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "22bf3136",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.16351354122161865,\n",
              "  'token': 31886,\n",
              "  'token_str': ' Python',\n",
              "  'sequence': 'This course will teach you all about natural language processing with Python'},\n",
              " {'score': 0.04778652638196945,\n",
              "  'token': 5136,\n",
              "  'token_str': ' ease',\n",
              "  'sequence': 'This course will teach you all about natural language processing with ease'},\n",
              " {'score': 0.02428208664059639,\n",
              "  'token': 49430,\n",
              "  'token_str': ' Clojure',\n",
              "  'sequence': 'This course will teach you all about natural language processing with Clojure'},\n",
              " {'score': 0.02377931773662567,\n",
              "  'token': 46948,\n",
              "  'token_str': ' Lua',\n",
              "  'sequence': 'This course will teach you all about natural language processing with Lua'},\n",
              " {'score': 0.02255992963910103,\n",
              "  'token': 38592,\n",
              "  'token_str': ' Haskell',\n",
              "  'sequence': 'This course will teach you all about natural language processing with Haskell'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlm_pipeline = pipeline(\"fill-mask\")\n",
        "mlm_pipeline(\"This course will teach you all about natural language processing with <mask>\", top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959ab9b5",
      "metadata": {},
      "source": [
        "### Translation\n",
        "The translation pipeline converts the input text from one language to the other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "55771237",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Die künstliche Intelligenz übernimmt die Welt'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator_pipeline = pipeline(\"translation_en_to_de\")\n",
        "translator_pipeline(\"AI is taking over the world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41852b4a",
      "metadata": {},
      "source": [
        "### Sentiment Analysis\n",
        "The Sentiment analysis pipeline returns the sentiment label and score given an input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a0c14348",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9996398687362671}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentiment_analysis_pipeline = pipeline(\"sentiment-analysis\")\n",
        "text = \"The movie was terrible but the music was good.\"\n",
        "sentiment_analysis_pipeline(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ed4d68",
      "metadata": {},
      "source": [
        "More information on NLP tasks by Huggingface can be found [here](https://huggingface.co/tasks)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python(cas-nlp)",
      "language": "python",
      "name": "cas-nlp"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
