{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab888773-758e-47e0-a7d8-3afa0fa33a44",
   "metadata": {},
   "source": [
    "# Retreival Augmented Generation \n",
    "In this notebook, we will learn the basics of RAG by asking questions from a chosen Knowledge Base.\n",
    "For the purpose of demonstration, we will choose, the [UBELIX](https://github.com/hpc-unibe-ch/hpc-unibe-ch.github.io/tree/main/docs) docs from github.\n",
    "1. At first, we need to get our data.\n",
    "```bash\n",
    "    mkdir Session7/data\n",
    "    cd Session7/data\n",
    "    curl -L -o repo.zip https://github.com/hpc-unibe-ch/hpc-unibe-ch.github.io/archive/refs/heads/main.zip\n",
    "    unzip repo.zip\n",
    "```\n",
    "2. We preprocess the text to create our vector database.\n",
    "3. We create our RAG bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed72e09-7087-4d5f-8f07-b9f92f430845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b7a6ec-8a0b-4923-aabf-04c252acccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_paths = glob.glob(pathname=\"data/**/*.[mM][dD]\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8dfa25-a16e-4311-acd4-af13e105d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_markdown(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md = f.read()\n",
    "        md = re.sub(r\"```.*?```\", \"\", md, flags=re.DOTALL)\n",
    "        return md\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 150) -> List[str]:\n",
    "    # simple, robust char chunking\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        chunk = text[i:i+chunk_size]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "        i += max(1, chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "def stable_id(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f342e2-b4a2-4586-8f9a-424c44b8b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_update_chroma(\n",
    "    md_paths: List[str],\n",
    "    persist_dir: str = \"./chroma_md\",\n",
    "    collection_name: str = \"md_rag\",\n",
    "    embed_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    chunk_size: int = 1000,\n",
    "    overlap: int = 150,\n",
    "    update: bool = False,                 # <-- NEW: only add new docs if True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    If persist_dir exists:\n",
    "      - update=False => load existing collection and return (no writes)\n",
    "      - update=True  => load existing collection and add missing chunks\n",
    "\n",
    "    If persist_dir does not exist:\n",
    "      - create and populate from md_paths (regardless of update flag)\n",
    "    \"\"\"\n",
    "    persist_exists = os.path.isdir(persist_dir) and any(os.scandir(persist_dir))\n",
    "\n",
    "    client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    # If the DB already exists and we don't want to update, just load and return.\n",
    "    if persist_exists and not update:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        embedder = SentenceTransformer(embed_model_name)\n",
    "        return {\"client\": client, \"collection\": collection, \"embedder\": embedder}\n",
    "\n",
    "    # Otherwise: create / get the collection and (optionally) add new items\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    embedder = SentenceTransformer(embed_model_name)\n",
    "\n",
    "    # If no paths provided and we're in update mode, just return loaded objects\n",
    "    if not md_paths:\n",
    "        return {\"client\": client, \"collection\": collection, \"embedder\": embedder}\n",
    "\n",
    "    docs_to_add, metas_to_add, ids_to_add = [], [], []\n",
    "\n",
    "    for p in md_paths:\n",
    "        md = read_markdown(p)\n",
    "        chunks = chunk_text(md, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        for j, c in enumerate(chunks):\n",
    "            cid = stable_id(f\"{p}::{j}::{c[:80]}\")\n",
    "            ids_to_add.append(cid)\n",
    "            docs_to_add.append(c)\n",
    "            metas_to_add.append({\"source\": p, \"chunk\": j})\n",
    "\n",
    "    # Avoid duplicate IDs (Chroma errors on duplicates)\n",
    "    existing = set()\n",
    "    B = 500\n",
    "    for i in range(0, len(ids_to_add), B):\n",
    "        batch_ids = ids_to_add[i : i + B]\n",
    "        got = collection.get(ids=batch_ids, include=[])\n",
    "        existing.update(got.get(\"ids\", []) or [])\n",
    "\n",
    "    new_docs, new_metas, new_ids = [], [], []\n",
    "    for d, m, i_ in zip(docs_to_add, metas_to_add, ids_to_add):\n",
    "        if i_ not in existing:\n",
    "            new_docs.append(d)\n",
    "            new_metas.append(m)\n",
    "            new_ids.append(i_)\n",
    "\n",
    "    if new_ids:\n",
    "        new_embs = embedder.encode(new_docs, normalize_embeddings=True, show_progress_bar=True)\n",
    "        new_embs = np.asarray(new_embs, dtype=\"float32\").tolist()\n",
    "\n",
    "        collection.add(ids=new_ids, documents=new_docs, metadatas=new_metas, embeddings=new_embs)\n",
    "\n",
    "    return {\"client\": client, \"collection\": collection, \"embedder\": embedder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35a26f-2b79-4767-9202-99725673ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(collection, embedder, query: str, k: int = 5):\n",
    "    qemb = embedder.encode([query], normalize_embeddings=True)\n",
    "    qemb = np.asarray(qemb, dtype=\"float32\").tolist()\n",
    "\n",
    "    res = collection.query(\n",
    "        query_embeddings=qemb,\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    out = []\n",
    "    for doc, meta, dist in zip(res[\"documents\"][0], res[\"metadatas\"][0], res[\"distances\"][0]):\n",
    "        out.append({\"text\": doc, \"meta\": meta, \"distance\": float(dist)})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e3a53c-6ba3-40ef-a88a-23c48e2abb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ca5b656-6518-4efa-b666-fcca496603b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_answer(query: str, collection, embedder, pipe, k: int = 5, max_new_tokens=1000,do_sample=True, temperature=0.2, top_p=0.9 ) -> str:\n",
    "    hits = retrieve(collection, embedder, query, k=k)\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{i+1}] ({h['meta']['source']}#chunk={h['meta']['chunk']})\\n{h['text']}\"\n",
    "        for i, h in enumerate(hits)\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "        Use ONLY the context to answer. If the answer is not in the context, say \"I don't know\".\n",
    "        Cite sources like [1], [2].\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=do_sample, temperature=temperature, top_p=top_p)\n",
    "    return out[0][\"generated_text\"][len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6f780a2-100f-4ab7-a6e8-aca0d2069495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def without_rag_answer(query: str, pipe, max_new_tokens=1000,do_sample=True, temperature=0.2, top_p=0.9 ) -> str:\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "        If you do not know the answer, say \"I don't know\".\n",
    "        Cite sources like [1], [2].        \n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    out = pipe(prompt, max_new_tokens=max_new_tokens, do_sample=do_sample, temperature=temperature, top_p=top_p)\n",
    "    return out[0][\"generated_text\"][len(prompt):].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9f1e98-eab7-4ef8-acc1-bf783b8d9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = build_or_update_chroma(\n",
    "    md_paths=md_paths,\n",
    "    persist_dir=\"./chroma_md\",\n",
    "    collection_name=\"md_rag\",\n",
    "    update=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e6d66d8-8601-4812-b63a-5ea204eb8df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15a4bff7bb840bbb4e5f4e263720731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def make_pipeline(model_name: str):\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    return pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n",
    "pipe = make_pipeline(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ccad307-4e91-4edd-9cae-52bf18be67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to submit a python script on Ubelix? I want to use Anaconda and I need a GPU.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd66dfe3-1211-4c33-bc99-a2b46bcf1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To submit a Python script on Ubelix using Anaconda and a GPU, follow these steps:\n",
      "\n",
      "1. First, make sure you have Anaconda installed on your local machine. If not, download and install it from the official Anaconda website: https://www.anaconda.com/products/individual\n",
      "\n",
      "2. Create a new environment for your project and install the necessary packages, including the GPU-compatible versions of TensorFlow and PyTorch. You can do this using the following commands:\n",
      "\n",
      "```bash\n",
      "conda create --name my_project\n",
      "conda activate my_project\n",
      "conda install -c anaconda tensorflow-gpu\n",
      "conda install -c anaconda pytorch\n",
      "```\n",
      "\n",
      "3. Write your Python script and make sure it works correctly on your local machine.\n",
      "\n",
      "4. SSH into the Ubelix cluster using the following command:\n",
      "\n",
      "```bash\n",
      "ssh your_username@ubelix.example.com\n",
      "```\n",
      "\n",
      "5. Once connected to the Ubelix cluster, create a new directory for your project:\n",
      "\n",
      "```bash\n",
      "mkdir my_project\n",
      "cd my_project\n",
      "```\n",
      "\n",
      "6. Transfer your Python script and any necessary data files from your local machine to the Ubelix cluster using the `scp` command:\n",
      "\n",
      "```bash\n",
      "scp my_script.py your_username@ubelix.example.com:my_project/\n",
      "scp data.csv your_username@ubelix.example.com:my_project/\n",
      "```\n",
      "\n",
      "7. Activate your environment on the Ubelix cluster:\n",
      "\n",
      "```bash\n",
      "conda activate my_project\n",
      "```\n",
      "\n",
      "8. Run your Python script:\n",
      "\n",
      "```bash\n",
      "python my_script.py\n",
      "```\n",
      "\n",
      "9. To submit the script as a job, you'll need to use the Slurm workload manager. First, create a Slurm script (e.g., `my_script.slurm`) with the following content:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "#SBATCH --job-name=my_job\n",
      "#SBATCH --time=0-04:00:00\n",
      "#SBATCH --partition=gpu\n",
      "#SBATCH --nodes=1\n",
      "#SBATCH --ntasks-per-node=1\n",
      "#SBATCH --cpus-per-task=8\n",
      "#SBATCH --mem=16GB\n",
      "#SBATCH --gres=gpu:1\n",
      "\n",
      "module load anaconda3\n",
      "source activate my_project\n",
      "\n",
      "python my_script.py\n",
      "```\n",
      "\n",
      "10. Submit the Slurm script:\n",
      "\n",
      "```bash\n",
      "sbatch my_script.slurm\n",
      "```\n",
      "\n",
      "Your job will now run on the Ubelix cluster with a GPU. You can monitor the status of your jobs using the `squeue` command.\n"
     ]
    }
   ],
   "source": [
    "print(without_rag_answer(query, pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a81cedbb-7631-4216-a0e4-55c147fdc8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To submit a Python script on UBELIX that uses Anaconda and requires a GPU, follow these steps:\n",
      "\n",
      "        1. Request an interactive job on a GPU node by submitting a SLURM job script. Here's an example script:\n",
      "\n",
      "        ```\n",
      "        #!/bin/bash\n",
      "        #SBATCH --job-name=my_job\n",
      "        #SBATCH --time=0-04:00:00\n",
      "        #SBATCH --partition=gpu\n",
      "        #SBATCH --gres=gpu:rtx3090:1\n",
      "        #SBATCH --ntasks=1\n",
      "        #SBATCH --cpus-per-task=1\n",
      "        #SBATCH --mem=16G\n",
      "\n",
      "        module load anaconda3\n",
      "        source activate myenv\n",
      "        python my_script.py\n",
      "        ```\n",
      "\n",
      "        2. Replace `my_job` with a name for your job, adjust the time, partition, GPU type, and other resources as needed.\n",
      "\n",
      "        3. Save the script as a file, for example `submit_job.sh`.\n",
      "\n",
      "        4. Submit the job with the `sbatch` command:\n",
      "\n",
      "        ```\n",
      "        sbatch submit_job.sh\n",
      "        ```\n",
      "\n",
      "        5. The job will start and run your Python script in an interactive shell on a GPU node with Anaconda activated in the environment `myenv`.\n",
      "\n",
      "        6. You can check the status of your job with the `squeue` command.\n",
      "\n",
      "        7. When the job is finished, you can find the output files in the working directory specified in the SLURM script.\n",
      "\n",
      "        8. If you need to install additional packages for your Python script, you can do so in the interactive shell or in the `myenv` environment before running the script.\n",
      "\n",
      "        9. For more information about using SLURM, GPUs, and Anaconda on UBELIX, consult the documentation:\n",
      "\n",
      "        - [GPUs](https://hpc-unibe-ch.github.io/docs/hpc-unibe-ch.github.io-main/docs/runjobs/scheduled-jobs/gpus.md)\n",
      "        - [Anaconda on UBELIX](https://hpc-unibe-ch.github.io/docs/hpc-unibe-ch.github.io-main/docs/software/packages/anaconda.md)\n"
     ]
    }
   ],
   "source": [
    "print(rag_answer(query, idx[\"collection\"], idx[\"embedder\"], pipe, k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5533f5c5-586e-4a16-abfc-c0f364d5eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To access Ubelix, you typically need to have an account with the service. Here's a general process:\n",
      "\n",
      "        1. Go to the Ubelix website.\n",
      "        2. Click on the \"Sign Up\" or \"Register\" button.\n",
      "        3. Fill in the required information, such as your name, email address, and password.\n",
      "        4. Agree to the terms and conditions.\n",
      "        5. Check your email for a confirmation message.\n",
      "        6. Click on the confirmation link to activate your account.\n",
      "        7. Log in with your email and password.\n",
      "\n",
      "Please note that the exact process may vary depending on the specific Ubelix service you are trying to access. For more detailed instructions, I recommend checking the Ubelix help center or contacting their customer support.\n",
      "\n",
      "Sources:\n",
      "[1] Ubelix - Sign Up: https://www.ubelix.com/signup\n",
      "[2] Ubelix - Help Center: https://www.ubelix.com/help-center/\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I get an access to Ubelix?\"\n",
    "print(without_rag_answer(query, pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "336e99a4-8ac3-4930-872b-f0f9586dae23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To access UBELIX, you need to be a researcher or student at the University of Bern with a staff, student or faculty Campus Account. Note that when using UBELIX, you accept and adhere to the Unibe IT Directives (Terms of Use), the HPC Operational concept as well as our Code of Conduct. To request the activation of your Campus Account, please send a request via [https://serviceportal.unibe.ch/hpc](https://serviceportal.unibe.ch/hpc) including:\n",
      "        - the title **HPC Account Activation**\n",
      "        - a brief description of what you want to use the cluster for\n",
      "\n",
      "        If you already have an account, you can log in to UBELIX using SSH client or web interface. If you don't have an account, you need to request it first.\n",
      "\n",
      "        References:\n",
      "        - [Access to UBELIX](data/hpc-unibe-ch.github.io-main/docs/firststeps/accessUBELIX.md)\n",
      "        - [SSH Keys](data/hpc-unibe-ch.github.io-main/docs/firststeps/SSH-keys.md)\n",
      "        - [Logging in with SSH client](data/hpc-unibe-ch.github.io-main/docs/firststeps/loggingin.md)\n",
      "        - [Logging in with web interface](data/hpc-unibe-ch.github.io-main/docs/firststeps/loggingin-webui.md)\n",
      "        - [Move data to UBELIX](data/hpc-unibe-ch.github.io-main/docs/firststeps/move-data.md)\n",
      "        - [Explore your options for using UBELIX](data/hpc-unibe-ch.github.io-main/docs/firststeps/nextsteps.md)\n",
      "        - [Help](data/hpc-unibe-ch.github.io-main/docs/firststeps/help.md)\n",
      "        - [Support](data/hpc-unibe-ch.github.io-main/docs/support/index.md)\n",
      "        - [Teaching with UBELIX](data/hpc-unibe-ch.github.io-main/docs/teaching.md)\n",
      "        - [Terms of Use](https://www.unibe.ch/universitaet/organisation/rechtliches/rechtssammlung/informatik/weisungen_und_beschluesse/index_ger.html)\n",
      "        - [Code of Conduct](../code-of-conduct.md)\n",
      "        - [HPC Operational concept](https://intern.unibe.ch/unibe/uniintern/content/e1883/e83075/e257675/e275902/e670170/BK_High-Performance-Computing-v1.6_ger.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(rag_answer(query, idx[\"collection\"], idx[\"embedder\"], pipe, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1d042-d8be-4b81-af57-35ddae4de13e",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "1. How can you assess the performance? For example, Test Set, Metric (Discuss)\n",
    "2. Choose and compare another model(Code). What model may be used improve the performance? (Discuss)\n",
    "3. How can you read in other document types? (PDFs for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0128bfcc-149d-4fa7-bd3d-6d6e57e98627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
